{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09d2aee-b2e5-406f-803f-c7657d414cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Water Pump Classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "# Load dataset (adjust path as needed)\n",
    "data_path = '../Data/train.csv'\n",
    "labels_path = '../Data/train_labels.csv' \n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "train = pd.read_csv(data_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f669d309-9c29-4645-9f1a-0b8970a29a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pumpit_clean(df :pd.core.frame.DataFrame):\n",
    "    # ``` This function requires: numpy, pandas, os,\n",
    "    # ```\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    # handle extra imports here\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    ## Numerical Variables\n",
    "\n",
    "    # amount_tsh: cap at 15000\n",
    "    df['amount_tsh'] = df['amount_tsh'].apply(lambda x: min(x, 15000))\n",
    "    # might want to replace this with bins\n",
    "    scaler = MinMaxScaler()\n",
    "    df['amount_tsh'] = scaler.fit_transform(df[['amount_tsh']]) # names kept consistent\n",
    "    # gps_height\n",
    "    # # Replace invalid gps_height values (e.g. 0 or negative)\n",
    "    df['gps_height'] = df['gps_height'].apply(lambda x: np.nan if x <= 0 else x)\n",
    "    # Fill using median per basin\n",
    "    df['gps_height'] = df.groupby('basin')['gps_height'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    # Fill any still missing using region median\n",
    "    df['gps_height'] = df.groupby('region')['gps_height'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    df['gps_height'] = scaler.fit_transform(df[['gps_height']])\n",
    "\n",
    "    # location:\n",
    "    df['longitude'] = df['longitude'].replace(0, np.nan)\n",
    "    df['latitude'] = df['latitude'].where(df['latitude'] < -0.5, np.nan) # too close to the equator\n",
    "    for i in ['latitude','longitude']:\n",
    "        df[i] = df.groupby('lga')[i].transform(lambda x: x.fillna(x.median))\n",
    "        df[i] = df.groupby('region')[i].transform(lambda x: x.fillna(x.median))\n",
    "\n",
    "    # Fill population using median by district_code\n",
    "    df['population'] = df.groupby('lga')['population'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    # Fill any still missing with median by region, then overall median\n",
    "    df['population'] = df.groupby('region')['population'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    df['population'] = df['population'].fillna(df.population.median)\n",
    "    # Bin the outcome, see how it behaves\n",
    "    df['population'] = pd.cut(df['population'], [-1,1,25,90,160,260,9999999], labels=[0,0.1,0.2,0.4,0.6,0.9])\n",
    "    df['population'] = df['population'].astype(float)\n",
    "    # Construction year ??\n",
    "    df['construction_year'] = df['construction_year'].replace(0, np.nan)\n",
    "    df['date_recorded'] = pd.to_datetime(df['date_recorded'])\n",
    "    df['recorded_year'] = df['date_recorded'].dt.year\n",
    "    #Impute using region + installer\n",
    "    df['construction_year'] = df.groupby(['region', 'installer'])['construction_year'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    #Impute using region only (for rows still missing)\n",
    "    df['construction_year'] = df.groupby('region')['construction_year'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    #Use recorded year - 5\n",
    "    df['construction_year'] = df['construction_year'].fillna(df['recorded_year'] - 5)\n",
    "    df['construction_year'] = scaler.fit_transform(df[['construction_year']])\n",
    "\n",
    "    ### Encode categorical variables\n",
    "\n",
    "    # Encode 'quantity' (and typo fix: 'insufficent' -> 'insufficient')\n",
    "    df['quantity'] = df['quantity'].replace({\n",
    "        'enough': 1,\n",
    "        'seasonal': 0.6,\n",
    "        'insufficient': 0.4,\n",
    "        'dry': 0,\n",
    "        'unknown': 0\n",
    "    })\n",
    "    df['quantity'] = df['quantity'].astype(float)\n",
    "    # Encode 'water_quality' as binary: good = 1, else 0\n",
    "    df['water_quality'] = np.where(df['water_quality'] == 'soft', 1, 0)\n",
    "    # Encode 'waterpoint_type' (1 = preferred type, 0 = everything else)\n",
    "    preferred_waterpoint = ['hand pump', 'communal standpipe']\n",
    "    df['waterpoint_type'] = df['waterpoint_type'].apply(lambda x: 1 if x in preferred_waterpoint else 0)\n",
    "    # Encode 'payment' as binary: never pay = 0, else = 1\n",
    "    df['payment'] = np.where(df['payment'] == 'never pay', 0, 1)\n",
    "    # Encode 'source' (1 = preferred sources, 0 = everything else)\n",
    "    preferred_sources = ['spring', 'river', 'rainwater harvesting']\n",
    "    df['source'] = df['source'].apply(lambda x: 1 if x in preferred_sources else 0)\n",
    "    # Encode 'payment' as binary: never pay = 0, else = 1\n",
    "    df['extraction_type_class'] = np.where(df['extraction_type_class'] == 'gravity', 0, 1)\n",
    "\n",
    "    #  Drop other columns and only keep these:\n",
    "    df = df[['amount_tsh',\n",
    "             'gps_height',\n",
    " #            'longitude',\n",
    " #            'latitude',\n",
    "             'population',\n",
    "             'construction_year',\n",
    "             'extraction_type_class',\n",
    "             'payment',\n",
    "            'water_quality',\n",
    "            'quantity',\n",
    "            'source',\n",
    "            'waterpoint_type',\n",
    "       ]]\n",
    "    df['populationXquantity'] = df['population'] * df['quantity']\n",
    "    df['waterpointXsource'] = df['waterpoint_type'] * df['source']\n",
    "    df['quantityXsource'] = df['quantity'] * df['source']\n",
    "    df['constr_yearXpopulation'] = df['construction_year'] * df['population']\n",
    "    df['waterpointXconst_year'] = df['waterpoint_type'] * df['construction_year']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "38315ffa-7ab9-4f9b-85af-3f35270c1f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59400 entries, 0 to 59399\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   amount_tsh              59400 non-null  float64\n",
      " 1   gps_height              59400 non-null  float64\n",
      " 2   population              59400 non-null  float64\n",
      " 3   construction_year       59400 non-null  float64\n",
      " 4   extraction_type_class   59400 non-null  int64  \n",
      " 5   payment                 59400 non-null  int64  \n",
      " 6   water_quality           59400 non-null  int64  \n",
      " 7   quantity                59400 non-null  float64\n",
      " 8   source                  59400 non-null  int64  \n",
      " 9   waterpoint_type         59400 non-null  int64  \n",
      " 10  populationXquantity     59400 non-null  float64\n",
      " 11  waterpointXsource       59400 non-null  int64  \n",
      " 12  quantityXsource         59400 non-null  float64\n",
      " 13  constr_yearXpopulation  59400 non-null  float64\n",
      " 14  waterpointXconst_year   59400 non-null  float64\n",
      "dtypes: float64(9), int64(6)\n",
      "memory usage: 6.8 MB\n"
     ]
    }
   ],
   "source": [
    "df = pumpit_clean(train)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d16c757f-dc38-4297-8a6a-fee337513ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "labels = pd.read_csv(os.path.join(labels_path))\n",
    "# separate labels into two variables\n",
    "y1 = labels.copy(deep=False)\n",
    "y2 = labels.copy(deep=False)\n",
    "y1.status_group = np.where(y1[\"status_group\"] == \"functional\", 1, 0)\n",
    "y2.status_group = np.where(y2[\"status_group\"] == \"non functional\", 1, 0)\n",
    "# make copy of training data\n",
    "y1 = y1.drop(columns=['id'])\n",
    "y2 = y2.drop(columns=['id'])\n",
    "X = df\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y1, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "336b5374-0651-4664-8bd4-cb30088825a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Type: <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      " Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('logreg',\n",
      "                 LogisticRegression(C=0.01, max_iter=1000, solver='saga'))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_logistic_model.pkl']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'logreg__C': [0.01, 0.1],\n",
    "    'logreg__penalty': ['l2'],\n",
    "    'logreg__solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=2, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)  # this is essential\n",
    "\n",
    "# Check best_estimator_\n",
    "print(\" Type:\", type(grid_search))\n",
    "print(\" Best Estimator:\", grid_search.best_estimator_)\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(grid_search.best_estimator_, 'best_logistic_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2445e625-2ff9-40b4-9922-5de1d4f314be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.60      0.65      8096\n",
      "           1       0.71      0.80      0.75      9724\n",
      "\n",
      "    accuracy                           0.71     17820\n",
      "   macro avg       0.71      0.70      0.70     17820\n",
      "weighted avg       0.71      0.71      0.71     17820\n",
      "\n",
      " Confusion Matrix:\n",
      "[[4881 3215]\n",
      " [1975 7749]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\" Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\" Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2ff980ce-1971-4428-914b-b1ec0c42cc30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Item wrong length 17820 instead of 59400.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_10631/1422616285.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m X2 = df\n\u001b[32m      2\u001b[39m X2.outcome1 = y1\n\u001b[32m      3\u001b[39m X2.outcome1 = y_test_pred\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X2 = X2[X2.outcome1 == \u001b[32m0\u001b[39m].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m X2.head()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# second round\u001b[39;00m\n\u001b[32m      7\u001b[39m X_train, X_test, y_train, y_test = train_test_split(\n",
      "\u001b[32m~/coding/venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4089\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.where(key)\n\u001b[32m   4090\u001b[39m \n\u001b[32m   4091\u001b[39m         \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[32m   4092\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m com.is_bool_indexer(key):\n\u001b[32m-> \u001b[39m\u001b[32m4093\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._getitem_bool_array(key)\n\u001b[32m   4094\u001b[39m \n\u001b[32m   4095\u001b[39m         \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[32m   4096\u001b[39m         \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n",
      "\u001b[32m~/coding/venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4139\u001b[39m                 UserWarning,\n\u001b[32m   4140\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   4141\u001b[39m             )\n\u001b[32m   4142\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(key) != len(self.index):\n\u001b[32m-> \u001b[39m\u001b[32m4143\u001b[39m             raise ValueError(\n\u001b[32m   4144\u001b[39m                 f\"Item wrong length {len(key)} instead of {len(self.index)}.\"\n\u001b[32m   4145\u001b[39m             )\n\u001b[32m   4146\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: Item wrong length 17820 instead of 59400."
     ]
    }
   ],
   "source": [
    "X2 = df\n",
    "X2.outcome1 = y1\n",
    "X2.outcome1 = y_test_pred\n",
    "X2 = X2[X2.outcome1 == 0].copy(deep=False)\n",
    "X2.head()\n",
    "# second round\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X2, y2, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a224f1e-6181-44e8-8ace-eedaa528be6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "167f68f3-0313-4208-ab24-353503a48e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In other news, here are the results with the added interaction variables\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(labels['status_group']) \n",
    "\n",
    "\n",
    "# Define features and target\n",
    "X = df\n",
    "y = y_encoded\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8f3a95b7-7578-4f6c-a3d2-32deb1e438f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Type: <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      " Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('logreg',\n",
      "                 LogisticRegression(C=0.1, max_iter=1000, solver='saga'))])\n",
      " Classification Report on Test Set:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.69      0.84      0.76      6452\n",
      "functional needs repair       0.00      0.00      0.00       863\n",
      "         non functional       0.69      0.60      0.64      4565\n",
      "\n",
      "               accuracy                           0.69     11880\n",
      "              macro avg       0.46      0.48      0.47     11880\n",
      "           weighted avg       0.64      0.69      0.66     11880\n",
      "\n",
      " Confusion Matrix:\n",
      "[[5442    0 1010]\n",
      " [ 634    0  229]\n",
      " [1822    0 2743]]\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)  # this is essential\n",
    "\n",
    "# Check best_estimator_\n",
    "print(\" Type:\", type(grid_search))\n",
    "print(\" Best Estimator:\", grid_search.best_estimator_)\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(grid_search.best_estimator_, 'best_logistic_model.pkl')\n",
    "\n",
    "# Evaluate on the test set\n",
    "\n",
    "y_test_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\" Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=le.classes_))\n",
    "\n",
    "print(\" Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178df912-6d8f-49b9-b768-30425aa0560b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a57da-afc5-4d84-85db-f78141fb270a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
